{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import json # to read json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/berry/Code/domain-adaptation-transformers-forQA\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/nl_squad_dev_filtered.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_json_to_dataframe_train(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n",
    "                           verbose = 1):\n",
    "    \"\"\"\n",
    "    input_file_path: path to the squad json file.\n",
    "    record_path: path to deepest level in json file default value is\n",
    "    ['data','paragraphs','qas','answers']\n",
    "    verbose: 0 to suppress it default is 1\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Reading the json file\")    \n",
    "    file = json.loads(open(input_file_path).read())\n",
    "    if verbose:\n",
    "        print(\"processing...\")\n",
    "    # parsing different level's in the json file\n",
    "    js = pd.io.json.json_normalize(file , record_path )\n",
    "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
    "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
    "    \n",
    "    #combining it into single dataframe\n",
    "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
    "    ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
    "    m['context'] = idx\n",
    "    js['q_idx'] = ndx\n",
    "    main = pd.concat([ m[['id','question','context']].set_index('id'),js.set_index('q_idx')],1,sort=False).reset_index()\n",
    "    main['c_id'] = main['context'].factorize()[0]\n",
    "    if verbose:\n",
    "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
    "        print(\"Done\")\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_json_to_dataframe_dev(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n",
    "                           verbose = 1):\n",
    "    \"\"\"\n",
    "    input_file_path: path to the squad json file.\n",
    "    record_path: path to deepest level in json file default value is\n",
    "    ['data','paragraphs','qas','answers']\n",
    "    verbose: 0 to suppress it default is 1\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Reading the json file\")    \n",
    "    file = json.loads(open(input_file_path).read())\n",
    "    if verbose:\n",
    "        print(\"processing...\")\n",
    "    # parsing different level's in the json file\n",
    "    js = pd.io.json.json_normalize(file , record_path )\n",
    "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
    "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
    "    \n",
    "    #combining it into single dataframe\n",
    "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
    "#     ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
    "    m['context'] = idx\n",
    "#     js['q_idx'] = ndx\n",
    "    main = m[['id','question','context','answers']].set_index('id').reset_index()\n",
    "    main['c_id'] = main['context'].factorize()[0]\n",
    "    if verbose:\n",
    "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
    "        print(\"Done\")\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev data\n",
    "input_file_path = '../data/nl_squad_dev_filtered.json'\n",
    "record_path = ['data','paragraphs','qas','answers']\n",
    "verbose = 0\n",
    "dev = squad_json_to_dataframe_dev(input_file_path=input_file_path,record_path=record_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pandas_df(local_cache_path=\".\", squad_version=\"v1.1\", file_split=\"train\"):\n",
    "    \"\"\"Loads the SQuAD dataset in pandas data frame.\n",
    "    Args:\n",
    "        local_cache_path (str, optional): Path to load the data from. If the file doesn't exist,\n",
    "            download it first. Defaults to the current directory.\n",
    "        squad_version (str, optional): Version of the SQuAD dataset, accepted values are: \n",
    "            \"v1.1\" and \"v2.0\". Defaults to \"v1.1\".\n",
    "        file_split (str, optional): Dataset split to load, accepted values are: \"train\" and \"dev\".\n",
    "            Defaults to \"train\".\n",
    "    \"\"\"\n",
    "\n",
    "    if file_split not in [\"train\", \"dev\"]:\n",
    "        raise ValueError(\"file_split should be either train or dev\")\n",
    "\n",
    "    URL = URL_DICT[squad_version][file_split]\n",
    "    file_name = URL.split(\"/\")[-1]\n",
    "    maybe_download(URL, file_name, local_cache_path)\n",
    "\n",
    "    file_path = os.path.join(local_cache_path, file_name)\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as reader:\n",
    "        input_data = json.load(reader)[\"data\"]\n",
    "\n",
    "    paragraph_text_list = []\n",
    "    question_text_list = []\n",
    "    answer_start_list = []\n",
    "    answer_text_list = []\n",
    "    qa_id_list = []\n",
    "    is_impossible_list = []\n",
    "    for entry in input_data:\n",
    "        for paragraph in entry[\"paragraphs\"]:\n",
    "            paragraph_text = paragraph[\"context\"]\n",
    "\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                qas_id = qa[\"id\"]\n",
    "                question_text = qa[\"question\"]\n",
    "                answer_offset = None\n",
    "                is_impossible = False\n",
    "\n",
    "                if squad_version == \"v2.0\":\n",
    "                    is_impossible = qa[\"is_impossible\"]\n",
    "\n",
    "                if file_split == \"train\":\n",
    "                    if (len(qa[\"answers\"]) != 1) and (not is_impossible):\n",
    "                        raise ValueError(\n",
    "                            \"For training, each question should have exactly 1 answer.\"\n",
    "                        )\n",
    "                    if not is_impossible:\n",
    "                        answer = qa[\"answers\"][0]\n",
    "                        orig_answer_text = answer[\"text\"]\n",
    "                        answer_offset = answer[\"answer_start\"]\n",
    "                    else:\n",
    "                        orig_answer_text = \"\"\n",
    "                else:\n",
    "                    if not is_impossible:\n",
    "                        orig_answer_text = []\n",
    "                        answer_offset = []\n",
    "                        for answer in qa[\"answers\"]:\n",
    "                            orig_answer_text.append(answer[\"text\"])\n",
    "                            answer_offset.append(answer[\"answer_start\"])\n",
    "                    else:\n",
    "                        orig_answer_text = \"\"\n",
    "\n",
    "                paragraph_text_list.append(paragraph_text)\n",
    "                question_text_list.append(question_text)\n",
    "                answer_start_list.append(answer_offset)\n",
    "                answer_text_list.append(orig_answer_text)\n",
    "                qa_id_list.append(qas_id)\n",
    "                is_impossible_list.append(is_impossible)\n",
    "\n",
    "    output_df = pd.DataFrame(\n",
    "        {\n",
    "            \"doc_text\": paragraph_text_list,\n",
    "            \"question_text\": question_text_list,\n",
    "            \"answer_start\": answer_start_list,\n",
    "            \"answer_text\": answer_text_list,\n",
    "            \"qa_id\": qa_id_list,\n",
    "            \"is_impossible\": is_impossible_list,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/microsoft/nlp-recipes/blob/master/utils_nlp/dataset/squad.py\n",
    "Ik ga dit vanavond doen, niet als ik in de zon zit. Het moet wel lukken hiermee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
