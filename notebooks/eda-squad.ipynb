{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://nlmatics.github.io/site_files/nick_post/squad_image.png\" alt=\"squad\" width=\"300\"/>\n",
    "\n",
    "# Dutch SQuAD 2.0 dataset\n",
    "*Note:* this dataset is a machine-translated version of the SQuAD v2.0 from Stanford University and can be found [here.](https://gitlab.com/niels.rouws/dutch-squad-v2.0) <br>\n",
    "\n",
    "**How to use:** <br>\n",
    "It is best to use Google Colab and run the notebook to get results. <br><br>\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to install packages if needed\n",
    "!pip install torch  torchvision -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import json \n",
    "import collections\n",
    "from pprint import pprint\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Dutch SQuAD2.0 dev set\n",
    "!wget -P data/squad/ https://gitlab.com/niels.rouws/dutch-squad-v2.0/-/raw/main/nl_squad_dev_filtered.json\n",
    "\n",
    "# Download the Dutch SQuAD2.0 train set\n",
    "!wget -P data/squad/ https://gitlab.com/niels.rouws/dutch-squad-v2.0/-/raw/main/nl_squad_train_filtered.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.data.processors.squad import SquadV2Processor\n",
    "\n",
    "# this processor loads the SQuAD2.0 dev set examples\n",
    "processor = SquadV2Processor()\n",
    "examples = processor.get_dev_examples(\"./data/squad/\", filename=\"nl_squad_dev_filtered.json\")\n",
    "print(len(examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this processor loads the SQuAD2.0 train set examples\n",
    "processor = SquadV2Processor()\n",
    "examples = processor.get_dev_examples(\"./data/squad/\", filename=\"nl_squad_train_filtered.json\")\n",
    "print(len(examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some maps to help us identify examples of interest\n",
    "qid_to_example_index = {example.qas_id: i for i, example in enumerate(examples)}\n",
    "qid_to_has_answer = {example.qas_id: bool(example.answers) for example in examples}\n",
    "answer_qids = [qas_id for qas_id, has_answer in qid_to_has_answer.items() if has_answer]\n",
    "no_answer_qids = [qas_id for qas_id, has_answer in qid_to_has_answer.items() if not has_answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for checking one example\n",
    "def display_example(qid):    \n",
    "    from pprint import pprint\n",
    "\n",
    "    idx = qid_to_example_index[qid]\n",
    "    q = examples[idx].question_text\n",
    "    c = examples[idx].context_text\n",
    "    a = [answer['text'] for answer in examples[idx].answers]\n",
    "    \n",
    "    print(f'Example {idx} of {len(examples)}\\n---------------------')\n",
    "    print(f\"Q: {q}\\n\")\n",
    "    print(\"Context:\")\n",
    "    pprint(c)\n",
    "    print(f\"\\nTrue Answers:\\n{a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_example(answer_qids[1300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_example(no_answer_qids[1254])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_json_to_dataframe_train(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n",
    "                           verbose = 1):\n",
    "    \"\"\"\n",
    "    input_file_path: path to the squad json file.\n",
    "    record_path: path to deepest level in json file default value is\n",
    "    ['data','paragraphs','qas','answers']\n",
    "    verbose: 0 to suppress it default is 1\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Reading the json file\")    \n",
    "    file = json.loads(open(input_file_path).read())\n",
    "    if verbose:\n",
    "        print(\"processing...\")\n",
    "    # parsing different level's in the json file\n",
    "    js = pd.io.json.json_normalize(file , record_path )\n",
    "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
    "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
    "    \n",
    "    #combining it into single dataframe\n",
    "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
    "    ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
    "    m['context'] = idx\n",
    "    js['q_idx'] = ndx\n",
    "    main = pd.concat([ m[['id','question','context']].set_index('id'),js.set_index('q_idx')],1,sort=False).reset_index()\n",
    "    main['c_id'] = main['context'].factorize()[0]\n",
    "    if verbose:\n",
    "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
    "        print(\"Done\")\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_json_to_dataframe_dev(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n",
    "                           verbose = 1):\n",
    "    \"\"\"\n",
    "    input_file_path: path to the squad json file.\n",
    "    record_path: path to deepest level in json file default value is\n",
    "    ['data','paragraphs','qas','answers']\n",
    "    verbose: 0 to suppress it default is 1\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Reading the json file\")    \n",
    "    file = json.loads(open(input_file_path).read())\n",
    "    if verbose:\n",
    "        print(\"processing...\")\n",
    "    # parsing different level's in the json file\n",
    "    js = pd.io.json.json_normalize(file , record_path )\n",
    "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
    "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
    "    \n",
    "    #combining it into single dataframe\n",
    "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
    "#     ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
    "    m['context'] = idx\n",
    "#     js['q_idx'] = ndx\n",
    "    main = m[['id','question','context','answers']].set_index('id').reset_index()\n",
    "    main['c_id'] = main['context'].factorize()[0]\n",
    "    if verbose:\n",
    "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
    "        print(\"Done\")\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev data\n",
    "input_file_path = '../data/nl_squad_dev_filtered.json'\n",
    "record_path = ['data','paragraphs','qas','answers']\n",
    "verbose = 0\n",
    "dev = squad_json_to_dataframe_dev(input_file_path=input_file_path,record_path=record_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pandas_df(local_cache_path=\".\", squad_version=\"v1.1\", file_split=\"train\"):\n",
    "    \"\"\"Loads the SQuAD dataset in pandas data frame.\n",
    "    Args:\n",
    "        local_cache_path (str, optional): Path to load the data from. If the file doesn't exist,\n",
    "            download it first. Defaults to the current directory.\n",
    "        squad_version (str, optional): Version of the SQuAD dataset, accepted values are: \n",
    "            \"v1.1\" and \"v2.0\". Defaults to \"v1.1\".\n",
    "        file_split (str, optional): Dataset split to load, accepted values are: \"train\" and \"dev\".\n",
    "            Defaults to \"train\".\n",
    "    \"\"\"\n",
    "\n",
    "    if file_split not in [\"train\", \"dev\"]:\n",
    "        raise ValueError(\"file_split should be either train or dev\")\n",
    "\n",
    "    URL = URL_DICT[squad_version][file_split]\n",
    "    file_name = URL.split(\"/\")[-1]\n",
    "    maybe_download(URL, file_name, local_cache_path)\n",
    "\n",
    "    file_path = os.path.join(local_cache_path, file_name)\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as reader:\n",
    "        input_data = json.load(reader)[\"data\"]\n",
    "\n",
    "    paragraph_text_list = []\n",
    "    question_text_list = []\n",
    "    answer_start_list = []\n",
    "    answer_text_list = []\n",
    "    qa_id_list = []\n",
    "    is_impossible_list = []\n",
    "    for entry in input_data:\n",
    "        for paragraph in entry[\"paragraphs\"]:\n",
    "            paragraph_text = paragraph[\"context\"]\n",
    "\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                qas_id = qa[\"id\"]\n",
    "                question_text = qa[\"question\"]\n",
    "                answer_offset = None\n",
    "                is_impossible = False\n",
    "\n",
    "                if squad_version == \"v2.0\":\n",
    "                    is_impossible = qa[\"is_impossible\"]\n",
    "\n",
    "                if file_split == \"train\":\n",
    "                    if (len(qa[\"answers\"]) != 1) and (not is_impossible):\n",
    "                        raise ValueError(\n",
    "                            \"For training, each question should have exactly 1 answer.\"\n",
    "                        )\n",
    "                    if not is_impossible:\n",
    "                        answer = qa[\"answers\"][0]\n",
    "                        orig_answer_text = answer[\"text\"]\n",
    "                        answer_offset = answer[\"answer_start\"]\n",
    "                    else:\n",
    "                        orig_answer_text = \"\"\n",
    "                else:\n",
    "                    if not is_impossible:\n",
    "                        orig_answer_text = []\n",
    "                        answer_offset = []\n",
    "                        for answer in qa[\"answers\"]:\n",
    "                            orig_answer_text.append(answer[\"text\"])\n",
    "                            answer_offset.append(answer[\"answer_start\"])\n",
    "                    else:\n",
    "                        orig_answer_text = \"\"\n",
    "\n",
    "                paragraph_text_list.append(paragraph_text)\n",
    "                question_text_list.append(question_text)\n",
    "                answer_start_list.append(answer_offset)\n",
    "                answer_text_list.append(orig_answer_text)\n",
    "                qa_id_list.append(qas_id)\n",
    "                is_impossible_list.append(is_impossible)\n",
    "\n",
    "    output_df = pd.DataFrame(\n",
    "        {\n",
    "            \"doc_text\": paragraph_text_list,\n",
    "            \"question_text\": question_text_list,\n",
    "            \"answer_start\": answer_start_list,\n",
    "            \"answer_text\": answer_text_list,\n",
    "            \"qa_id\": qa_id_list,\n",
    "            \"is_impossible\": is_impossible_list,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/microsoft/nlp-recipes/blob/master/utils_nlp/dataset/squad.py\n",
    "Ik ga dit vanavond doen, niet als ik in de zon zit. Het moet wel lukken hiermee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/nl_squad_dev_filtered.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.json_normalize('../data/nl_squad_dev_filtered.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('../data/squad/nl_squad_dev_filtered.json')\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "  \n",
    "# Iterating through the json\n",
    "# list\n",
    "for i in data['data']:\n",
    "    print(i)\n",
    "  \n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.json_normalize(data, max_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.json_normalize(data, record_path=['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.json_normalize(\n",
    "    data, \n",
    "    record_path =['data'],\n",
    "    meta=['paragraphs', 'qas'],\n",
    "    errors='ignore'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install utils_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "#from utils_nlp.dataset.url_utils import maybe_download\n",
    "\n",
    "URL_DICT = {\n",
    "    \"v1.1\": {\n",
    "        \"train\": \"https://raw.githubusercontent.com/rajpurkar/SQuAD-explorer/\"\n",
    "        \"master/dataset/train-v1.1.json\",\n",
    "        \"dev\": \"https://raw.githubusercontent.com/rajpurkar/SQuAD-explorer/\"\n",
    "        \"master/dataset/dev-v1.1.json\",\n",
    "    },\n",
    "    \"v2.0\": {\n",
    "        \"train\": \"https://raw.githubusercontent.com/rajpurkar/SQuAD-explorer/\"\n",
    "        \"master/dataset/train-v2.0.json\",\n",
    "        \"dev\": \"https://raw.githubusercontent.com/rajpurkar/SQuAD-explorer/\"\n",
    "        \"master/dataset/dev-v2.0.json\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def load_pandas_df(local_cache_path=\".\", squad_version=\"v1.1\", file_split=\"train\"):\n",
    "    \"\"\"Loads the SQuAD dataset in pandas data frame.\n",
    "    Args:\n",
    "        local_cache_path (str, optional): Path to load the data from. If the file doesn't exist,\n",
    "            download it first. Defaults to the current directory.\n",
    "        squad_version (str, optional): Version of the SQuAD dataset, accepted values are: \n",
    "            \"v1.1\" and \"v2.0\". Defaults to \"v1.1\".\n",
    "        file_split (str, optional): Dataset split to load, accepted values are: \"train\" and \"dev\".\n",
    "            Defaults to \"train\".\n",
    "    \"\"\"\n",
    "\n",
    "    if file_split not in [\"train\", \"dev\"]:\n",
    "        raise ValueError(\"file_split should be either train or dev\")\n",
    "\n",
    "    URL = URL_DICT[squad_version][file_split]\n",
    "    file_name = URL.split(\"/\")[-1]\n",
    "    #maybe_download(URL, file_name, local_cache_path)\n",
    "\n",
    "    file_path = os.path.join(local_cache_path, file_name)\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as reader:\n",
    "        input_data = json.load(reader)[\"data\"]\n",
    "\n",
    "    paragraph_text_list = []\n",
    "    question_text_list = []\n",
    "    answer_start_list = []\n",
    "    answer_text_list = []\n",
    "    qa_id_list = []\n",
    "    is_impossible_list = []\n",
    "    for entry in input_data:\n",
    "        for paragraph in entry[\"paragraphs\"]:\n",
    "            paragraph_text = paragraph[\"context\"]\n",
    "\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                qas_id = qa[\"id\"]\n",
    "                question_text = qa[\"question\"]\n",
    "                answer_offset = None\n",
    "                is_impossible = False\n",
    "\n",
    "                if squad_version == \"v2.0\":\n",
    "                    is_impossible = qa[\"is_impossible\"]\n",
    "\n",
    "                if file_split == \"train\":\n",
    "                    if (len(qa[\"answers\"]) != 1) and (not is_impossible):\n",
    "                        raise ValueError(\n",
    "                            \"For training, each question should have exactly 1 answer.\"\n",
    "                        )\n",
    "                    if not is_impossible:\n",
    "                        answer = qa[\"answers\"][0]\n",
    "                        orig_answer_text = answer[\"text\"]\n",
    "                        answer_offset = answer[\"answer_start\"]\n",
    "                    else:\n",
    "                        orig_answer_text = \"\"\n",
    "                else:\n",
    "                    if not is_impossible:\n",
    "                        orig_answer_text = []\n",
    "                        answer_offset = []\n",
    "                        for answer in qa[\"answers\"]:\n",
    "                            orig_answer_text.append(answer[\"text\"])\n",
    "                            answer_offset.append(answer[\"answer_start\"])\n",
    "                    else:\n",
    "                        orig_answer_text = \"\"\n",
    "\n",
    "                paragraph_text_list.append(paragraph_text)\n",
    "                question_text_list.append(question_text)\n",
    "                answer_start_list.append(answer_offset)\n",
    "                answer_text_list.append(orig_answer_text)\n",
    "                qa_id_list.append(qas_id)\n",
    "                is_impossible_list.append(is_impossible)\n",
    "\n",
    "    output_df = pd.DataFrame(\n",
    "        {\n",
    "            \"doc_text\": paragraph_text_list,\n",
    "            \"question_text\": question_text_list,\n",
    "            \"answer_start\": answer_start_list,\n",
    "            \"answer_text\": answer_text_list,\n",
    "            \"qa_id\": qa_id_list,\n",
    "            \"is_impossible\": is_impossible_list,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = load_pandas_df(local_cache_path=\"../data/squad\", squad_version=\"v2.0\", file_split=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Snippet for Top Non-Stopwords Barchart\n",
    "\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from collections import  Counter\n",
    "\n",
    "def plot_top_non_stopwords_barchart(text):\n",
    "    stop=set(stopwords.words('dutch'))\n",
    "    \n",
    "    new= text.str.split()\n",
    "    new=new.values.tolist()\n",
    "    corpus=[word for i in new for word in i]\n",
    "\n",
    "    counter=Counter(corpus)\n",
    "    most=counter.most_common()\n",
    "    x, y=[], []\n",
    "    for word,count in most[:40]:\n",
    "        if (word not in stop):\n",
    "            x.append(word)\n",
    "            y.append(count)\n",
    "            \n",
    "    sns.barplot(x=y,y=x).set(title='Top Non-Stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_non_stopwords_barchart(df_dev['question_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev['question_text'].str.split().\\\n",
    "    map(lambda x: len(x)).\\\n",
    "    hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer contains brackets, because it is a list of strings\n",
    "# lets remove them\n",
    "#df_dev['answer_text'] = df_dev['answer_text'].apply(lambda x: x.replace('[', ''))\n",
    "#df_dev['answer_text'] = df_dev['answer_text'].apply(lambda x: x.replace(']', ''))\n",
    "\n",
    "df_dev['answer_text'] = df_dev['answer_text'].replace(to_replace=r'[', value='', regex=True)\n",
    "df_dev['answer_text'] = df_dev['answer_text'].replace(to_replace=r']', value='', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this chart is not right, due to float types\n",
    "df_dev['answer_text'].str.split().\\\n",
    "    map(lambda x: len(str(x))).\\\n",
    "    hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_character_length_histogram(text):\n",
    "    text.str.len().\\\n",
    "        hist().set(xlabel='Character length', ylabel='Count', title='Character length histogram (Context)')\n",
    "\n",
    "plot_character_length_histogram(df_dev['answer_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_character_length_histogram(text):\n",
    "    text.str.len().\\\n",
    "        hist().set(xlabel='Character length', ylabel='Count', title='Character length histogram (Context)')\n",
    "\n",
    "plot_character_length_histogram(df_dev['question_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_character_length_histogram(text):\n",
    "    text.str.len().\\\n",
    "        hist().set(xlabel='Character length', ylabel='Count', title='Character length histogram (Context)')\n",
    "\n",
    "plot_character_length_histogram(df_dev['doc_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fba33e78b77f025177f80ea29759b55d5960913694c1540010dc608131b64ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
