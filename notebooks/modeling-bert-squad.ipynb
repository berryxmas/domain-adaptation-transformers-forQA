{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://nlmatics.github.io/site_files/nick_post/squad_image.png\" alt=\"squad\" width=\"300\"/>\n",
    "\n",
    "# Dutch SQuAD 2.0 dataset\n",
    "*Note:* this dataset is a machine-translated version of the SQuAD v2.0 from Stanford University and can be found [here.](https://gitlab.com/niels.rouws/dutch-squad-v2.0) <br>\n",
    "\n",
    "**How to use:** <br>\n",
    "It is best to use Google Colab and run the notebook to get results. <br><br>\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook:\n",
    "- Three Bert-based models are tested and evaluated\n",
    "  - MBert\n",
    "  - Bertje\n",
    "  - Robbert\n",
    "- Hyperparameters are checked and tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using Colab, install necessary libraries\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "import time\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Dutch SQuAD2.0 dev set\n",
    "!wget -P data/squad/ https://gitlab.com/niels.rouws/dutch-squad-v2.0/-/raw/main/nl_squad_dev_filtered.json\n",
    "\n",
    "# Download the Dutch SQuAD2.0 train set\n",
    "!wget -P data/squad/ https://gitlab.com/niels.rouws/dutch-squad-v2.0/-/raw/main/nl_squad_train_filtered.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import run squad pipeline from HF\n",
    "%load models/run_squad.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# model: bert-base-multilingual-cased\n",
    "# without training\n",
    "# without domain adaptation\n",
    "##########\n",
    "!python /content/run_squad.py  \\\n",
    "    --model_type bert   \\\n",
    "    --model_name_or_path bert-base-multilingual-cased  \\\n",
    "    --output_dir models/bert/bert-base-multilingual-cased \\\n",
    "    --data_dir  data \\\n",
    "    --train_file policyqa-train.json   \\\n",
    "    --predict_file policyqa-test.json   \\\n",
    "    --do_eval   \\\n",
    "    --do_lower_case  \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --learning_rate 3.6e-06 \\\n",
    "    --per_gpu_eval_batch_size 12   \\\n",
    "    --max_seq_length 384   \\\n",
    "    --doc_stride 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# model: bert-base-multilingual-cased\n",
    "# with training\n",
    "# without domain adaptation\n",
    "##########\n",
    "!python /content/run_squad.py  \\\n",
    "    --model_type bert   \\\n",
    "    --model_name_or_path bert-base-multilingual-cased  \\\n",
    "    --output_dir models/bert/bert-base-multilingual-cased \\\n",
    "    --data_dir  data \\\n",
    "    --train_file policyqa-train.json   \\\n",
    "    --predict_file policyqa-test.json   \\\n",
    "    --do_train   \\\n",
    "    --do_eval   \\\n",
    "    --do_lower_case  \\\n",
    "    --per_gpu_eval_batch_size 12   \\\n",
    "    --max_seq_length 384   \\\n",
    "    --doc_stride 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# model: bert-base-multilingual-cased\n",
    "# without training\n",
    "# with domain adaptation / hyperparameter tuning\n",
    "##########\n",
    "!python /content/run_squad.py  \\\n",
    "    --model_type bert   \\\n",
    "    --model_name_or_path bert-base-multilingual-cased  \\\n",
    "    --output_dir models/bert/bert-base-multilingual-cased \\\n",
    "    --data_dir  data \\\n",
    "    --train_file policyqa-train.json   \\\n",
    "    --predict_file policyqa-test.json   \\\n",
    "    --do_eval   \\\n",
    "    --do_lower_case  \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --learning_rate 3.6e-06 \\\n",
    "    --per_gpu_eval_batch_size 12   \\\n",
    "    --max_seq_length 384   \\\n",
    "    --doc_stride 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# model: bert-base-multilingual-cased\n",
    "# with training\n",
    "# with domain adaptation / hyperparameter tuning\n",
    "##########\n",
    "!python /content/run_squad.py  \\\n",
    "    --model_type bert   \\\n",
    "    --model_name_or_path bert-base-multilingual-cased  \\\n",
    "    --output_dir models/bert/bert-base-multilingual-cased \\\n",
    "    --data_dir  data \\\n",
    "    --train_file policyqa-train.json   \\\n",
    "    --predict_file policyqa-test.json   \\\n",
    "    --do_train   \\\n",
    "    --do_eval   \\\n",
    "    --do_lower_case  \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --learning_rate 3.6e-06 \\\n",
    "    --per_gpu_eval_batch_size 12   \\\n",
    "    --max_seq_length 384   \\\n",
    "    --doc_stride 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MBert, finetuned on Dutch SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# model: bert-base-multilingual-cased-finetuned-dutch-squad2\n",
    "# without training\n",
    "# without domain adaptation\n",
    "##########\n",
    "!python /content/run_squad.py  \\\n",
    "    --model_type bert   \\\n",
    "    --model_name_or_path henryk/bert-base-multilingual-cased-finetuned-dutch-squad2  \\\n",
    "    --output_dir models/bert/henryk/bert-base-multilingual-cased-finetuned-dutch-squad2 \\\n",
    "    --data_dir  data \\\n",
    "    --predict_file policyqa-test.json   \\\n",
    "    --do_eval   \\\n",
    "    --do_lower_case  \\\n",
    "    --per_gpu_eval_batch_size 12   \\\n",
    "    --max_seq_length 384   \\\n",
    "    --doc_stride 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# model: bert-base-multilingual-cased-finetuned-dutch-squad2\n",
    "# with training\n",
    "# without domain adaptation\n",
    "##########\n",
    "!python /content/run_squad.py  \\\n",
    "    --model_type bert   \\\n",
    "    --model_name_or_path henryk/bert-base-multilingual-cased-finetuned-dutch-squad2  \\\n",
    "    --output_dir models/bert/henryk/bert-base-multilingual-cased-finetuned-dutch-squad2 \\\n",
    "    --data_dir  data \\\n",
    "    --train_file policyqa-train.json   \\\n",
    "    --predict_file policyqa-test.json   \\\n",
    "    --do_train   \\\n",
    "    --do_eval   \\\n",
    "    --do_lower_case  \\\n",
    "    --per_gpu_eval_batch_size 12   \\\n",
    "    --max_seq_length 384   \\\n",
    "    --doc_stride 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# model: bert-base-multilingual-cased-finetuned-dutch-squad2\n",
    "# without training\n",
    "# with domain adaptation / hyperparameter tuning\n",
    "##########\n",
    "!python /content/run_squad.py  \\\n",
    "    --model_type bert   \\\n",
    "    --model_name_or_path henryk/bert-base-multilingual-cased-finetuned-dutch-squad2  \\\n",
    "    --output_dir models/bert/henryk/bert-base-multilingual-cased-finetuned-dutch-squad2 \\\n",
    "    --data_dir  data \\\n",
    "    --predict_file policyqa-test.json   \\\n",
    "    --do_eval   \\\n",
    "    --do_lower_case  \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --learning_rate 3.6e-06 \\\n",
    "    --per_gpu_eval_batch_size 12   \\\n",
    "    --max_seq_length 384   \\\n",
    "    --doc_stride 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# model: bert-base-multilingual-cased-finetuned-dutch-squad2\n",
    "# with training\n",
    "# with domain adaptation / hyperparameter tuning\n",
    "##########\n",
    "!python /content/run_squad.py  \\\n",
    "    --model_type bert   \\\n",
    "    --model_name_or_path henryk/bert-base-multilingual-cased-finetuned-dutch-squad2  \\\n",
    "    --output_dir models/bert/henryk/bert-base-multilingual-cased-finetuned-dutch-squad2 \\\n",
    "    --data_dir  data \\\n",
    "    --train_file policyqa-train.json   \\\n",
    "    --predict_file policyqa-test.json   \\\n",
    "    --do_train   \\\n",
    "    --do_eval   \\\n",
    "    --do_lower_case  \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --learning_rate 3.6e-06 \\\n",
    "    --per_gpu_eval_batch_size 12   \\\n",
    "    --max_seq_length 384   \\\n",
    "    --doc_stride 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bertje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# model: GroNLP/bert-base-dutch-cased\n",
    "# without training\n",
    "# without domain adaptation\n",
    "##########\n",
    "!python /content/run_squad.py  \\\n",
    "    --model_type bert   \\\n",
    "    --model_name_or_path GroNLP/bert-base-dutch-cased  \\\n",
    "    --output_dir models/bert/GroNLP/bert-base-dutch-cased \\\n",
    "    --data_dir  data \\\n",
    "    --train_file policyqa-train.json   \\\n",
    "    --predict_file policyqa-test.json   \\\n",
    "    --do_eval   \\\n",
    "    --do_lower_case  \\\n",
    "    --per_gpu_eval_batch_size 12   \\\n",
    "    --max_seq_length 384   \\\n",
    "    --doc_stride 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# model: GroNLP/bert-base-dutch-cased\n",
    "# with training\n",
    "# without domain adaptation\n",
    "##########\n",
    "!python /content/run_squad.py  \\\n",
    "    --model_type bert   \\\n",
    "    --model_name_or_path GroNLP/bert-base-dutch-cased  \\\n",
    "    --output_dir models/bert/GroNLP/bert-base-dutch-cased \\\n",
    "    --data_dir  data \\\n",
    "    --train_file policyqa-train.json   \\\n",
    "    --predict_file policyqa-test.json   \\\n",
    "    --do_train   \\\n",
    "    --do_eval   \\\n",
    "    --do_lower_case  \\\n",
    "    --per_gpu_eval_batch_size 12   \\\n",
    "    --max_seq_length 384   \\\n",
    "    --doc_stride 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# model: GroNLP/bert-base-dutch-cased\n",
    "# without training\n",
    "# with domain adaptation / hyperparameter tuning\n",
    "##########\n",
    "!python /content/run_squad.py  \\\n",
    "    --model_type bert   \\\n",
    "    --model_name_or_path GroNLP/bert-base-dutch-cased  \\\n",
    "    --output_dir models/bert/GroNLP/bert-base-dutch-cased \\\n",
    "    --data_dir  data \\\n",
    "    --train_file policyqa-train.json   \\\n",
    "    --predict_file policyqa-test.json   \\\n",
    "    --do_eval   \\\n",
    "    --do_lower_case  \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --learning_rate 3.6e-06 \\\n",
    "    --per_gpu_eval_batch_size 12   \\\n",
    "    --max_seq_length 384   \\\n",
    "    --doc_stride 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# model: GroNLP/bert-base-dutch-cased\n",
    "# with training\n",
    "# with domain adaptation / hyperparameter tuning\n",
    "##########\n",
    "!python /content/run_squad.py  \\\n",
    "    --model_type bert   \\\n",
    "    --model_name_or_path GroNLP/bert-base-dutch-cased  \\\n",
    "    --output_dir models/bert/GroNLP/bert-base-dutch-cased \\\n",
    "    --data_dir  data \\\n",
    "    --train_file policyqa-train.json   \\\n",
    "    --predict_file policyqa-test.json   \\\n",
    "    --do_train   \\\n",
    "    --do_eval   \\\n",
    "    --do_lower_case  \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --learning_rate 3.6e-06 \\\n",
    "    --per_gpu_eval_batch_size 12   \\\n",
    "    --max_seq_length 384   \\\n",
    "    --doc_stride 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Robbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# model: pdelobelle/robbert-v2-dutch-base\n",
    "# without training\n",
    "# without domain adaptation\n",
    "##########\n",
    "!python /content/run_squad.py  \\\n",
    "    --model_type bert   \\\n",
    "    --model_name_or_path pdelobelle/robbert-v2-dutch-base  \\\n",
    "    --output_dir models/bert/pdelobelle/robbert-v2-dutch-base \\\n",
    "    --data_dir  data \\\n",
    "    --train_file policyqa-train.json   \\\n",
    "    --predict_file policyqa-test.json   \\\n",
    "    --do_eval   \\\n",
    "    --do_lower_case  \\\n",
    "    --per_gpu_eval_batch_size 12   \\\n",
    "    --max_seq_length 384   \\\n",
    "    --doc_stride 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# model: pdelobelle/robbert-v2-dutch-base\n",
    "# with training\n",
    "# without domain adaptation\n",
    "##########\n",
    "!python /content/run_squad.py  \\\n",
    "    --model_type bert   \\\n",
    "    --model_name_or_path pdelobelle/robbert-v2-dutch-base  \\\n",
    "    --output_dir models/bert/pdelobelle/robbert-v2-dutch-base \\\n",
    "    --data_dir  data \\\n",
    "    --train_file policyqa-train.json   \\\n",
    "    --predict_file policyqa-test.json   \\\n",
    "    --do_train   \\\n",
    "    --do_eval   \\\n",
    "    --do_lower_case  \\\n",
    "    --per_gpu_eval_batch_size 12   \\\n",
    "    --max_seq_length 384   \\\n",
    "    --doc_stride 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# model: pdelobelle/robbert-v2-dutch-base\n",
    "# without training\n",
    "# with domain adaptation / hyperparameter tuning\n",
    "##########\n",
    "!python /content/run_squad.py  \\\n",
    "    --model_type bert   \\\n",
    "    --model_name_or_path pdelobelle/robbert-v2-dutch-base  \\\n",
    "    --output_dir models/bert/pdelobelle/robbert-v2-dutch-base \\\n",
    "    --data_dir  data \\\n",
    "    --train_file policyqa-train.json   \\\n",
    "    --predict_file policyqa-test.json   \\\n",
    "    --do_eval   \\\n",
    "    --do_lower_case  \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --learning_rate 3.6e-06 \\\n",
    "    --per_gpu_eval_batch_size 12   \\\n",
    "    --max_seq_length 384   \\\n",
    "    --doc_stride 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# model: pdelobelle/robbert-v2-dutch-base\n",
    "# with training\n",
    "# with domain adaptation / hyperparameter tuning\n",
    "##########\n",
    "!python /content/run_squad.py  \\\n",
    "    --model_type bert   \\\n",
    "    --model_name_or_path pdelobelle/robbert-v2-dutch-base  \\\n",
    "    --output_dir models/bert/pdelobelle/robbert-v2-dutch-base \\\n",
    "    --data_dir  data \\\n",
    "    --train_file policyqa-train.json   \\\n",
    "    --predict_file policyqa-test.json   \\\n",
    "    --do_train   \\\n",
    "    --do_eval   \\\n",
    "    --do_lower_case  \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --learning_rate 3.6e-06 \\\n",
    "    --per_gpu_eval_batch_size 12   \\\n",
    "    --max_seq_length 384   \\\n",
    "    --doc_stride 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model\n",
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"henryk/bert-base-multilingual-cased-finetuned-dutch-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for finding optimal hyperparameters using LLRD\n",
    "def AdamW_LLRD(model):\n",
    "    \n",
    "    opt_parameters = []    # To be passed to the optimizer (only parameters of the layers you want to update).\n",
    "    named_parameters = list(model.named_parameters()) \n",
    "        \n",
    "    # According to AAAMLP book by A. Thakur, we generally do not use any decay \n",
    "    # for bias and LayerNorm.weight layers.\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    init_lr = 3.5e-6 \n",
    "    head_lr = 3.6e-6\n",
    "    lr = init_lr\n",
    "    \n",
    "    # === Pooler and regressor ======================================================  \n",
    "    \n",
    "    params_0 = [p for n,p in named_parameters if (\"pooler\" in n or \"regressor\" in n) \n",
    "                and any(nd in n for nd in no_decay)]\n",
    "    params_1 = [p for n,p in named_parameters if (\"pooler\" in n or \"regressor\" in n)\n",
    "                and not any(nd in n for nd in no_decay)]\n",
    "    \n",
    "    head_params = {\"params\": params_0, \"lr\": head_lr, \"weight_decay\": 0.0}    \n",
    "    opt_parameters.append(head_params)\n",
    "        \n",
    "    head_params = {\"params\": params_1, \"lr\": head_lr, \"weight_decay\": 0.01}    \n",
    "    opt_parameters.append(head_params)\n",
    "                \n",
    "    # === 12 Hidden layers ==========================================================\n",
    "    \n",
    "    for layer in range(11,-1,-1):        \n",
    "        params_0 = [p for n,p in named_parameters if f\"encoder.layer.{layer}.\" in n \n",
    "                    and any(nd in n for nd in no_decay)]\n",
    "        params_1 = [p for n,p in named_parameters if f\"encoder.layer.{layer}.\" in n \n",
    "                    and not any(nd in n for nd in no_decay)]\n",
    "        \n",
    "        layer_params = {\"params\": params_0, \"lr\": lr, \"weight_decay\": 0.0}\n",
    "        opt_parameters.append(layer_params)   \n",
    "                            \n",
    "        layer_params = {\"params\": params_1, \"lr\": lr, \"weight_decay\": 0.01}\n",
    "        opt_parameters.append(layer_params)       \n",
    "        \n",
    "        lr *= 0.9     \n",
    "        \n",
    "    # === Embeddings layer ==========================================================\n",
    "    \n",
    "    params_0 = [p for n,p in named_parameters if \"embeddings\" in n \n",
    "                and any(nd in n for nd in no_decay)]\n",
    "    params_1 = [p for n,p in named_parameters if \"embeddings\" in n\n",
    "                and not any(nd in n for nd in no_decay)]\n",
    "    \n",
    "    embed_params = {\"params\": params_0, \"lr\": lr, \"weight_decay\": 0.0} \n",
    "    opt_parameters.append(embed_params)\n",
    "        \n",
    "    embed_params = {\"params\": params_1, \"lr\": lr, \"weight_decay\": 0.01} \n",
    "    opt_parameters.append(embed_params)        \n",
    "    \n",
    "    return transformers.AdamW(opt_parameters, lr=init_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdamW_LLRD(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this approach, we checked the optimal hyperparameters for every model. <br>\n",
    "Due to time constraints and computational power, not all options were tested. <br>\n",
    "Testing of all optimal hyperparameter groups can be seen as Future Work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
